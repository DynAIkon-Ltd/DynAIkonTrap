<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>DynAIkonTrap.comms API documentation</title>
<meta name="description" content="An interface for writing animal frames to disk or sending them to a server. The `AbstractOutput` combines a frame(s) with the most appropriate sensor â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>DynAIkonTrap.comms</code></h1>
</header>
<section id="section-intro">
<p>An interface for writing animal frames to disk or sending them to a server. The <code><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></code> combines a frame(s) with the most appropriate sensor log(s) and outputs these.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># DynAIkonTrap is an AI-infused camera trapping software package.
# Copyright (C) 2020 Miklas Riechmann

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.
&#34;&#34;&#34;
An interface for writing animal frames to disk or sending them to a server. The `AbstractOutput` combines a frame(s) with the most appropriate sensor log(s) and outputs these.
&#34;&#34;&#34;
from multiprocessing import Process
from typing import Dict, IO, Tuple, List, Union
from tempfile import NamedTemporaryFile
from io import StringIO
from datetime import datetime, timezone
from pathlib import Path
from os import listdir
from os.path import join
from json import dump, dumps

from requests import post
from requests.exceptions import HTTPError, ConnectionError
from numpy import asarray
import cv2  # pdoc3 can&#39;t handle importing individual OpenCV functions

from DynAIkonTrap.filtering import Filter
from DynAIkonTrap.sensor import SensorLog, SensorLogs, Reading
from DynAIkonTrap.logging import get_logger
from DynAIkonTrap.settings import (
    OutputMode,
    SenderSettings,
    OutputFormat,
    OutputSettings,
    WriterSettings,
)

logger = get_logger(__name__)


class VideoCaption:
    &#34;&#34;&#34;Class to aid in generating captions for video output. The captions are based on the logged sensor readings.&#34;&#34;&#34;

    def __init__(self, sensor_logs: SensorLogs, framerate: float):
        &#34;&#34;&#34;
        Args:
            sensor_logs (SensorLogs): The object containing the log of sensor readings
            framerate (float): Camera framerate
        &#34;&#34;&#34;
        self._sensor_logs = sensor_logs
        self._framerate = framerate

    def _generate_captions_dict(self, timestamps: List[float]) -&gt; Dict:
        &#34;&#34;&#34;NOTE: If sensor readings do not line up with frame capturing, there may be slight off-by-one style errors.&#34;&#34;&#34;
        captions = {}
        for frame_number, timestamp in enumerate(timestamps):
            # Retrieve the corresponding log
            log = self._sensor_logs.get(timestamp)

            # Get existing or create new caption with this log
            key = int(
                frame_number // (self._framerate * self._sensor_logs.read_interval)
            )
            caption = captions.get(
                key,
                {
                    &#39;start&#39;: frame_number,
                    &#39;stop&#39;: frame_number,
                    &#39;log&#39;: log,
                },
            )

            # Extend caption duration for a subsequent frame using the same log
            caption[&#39;stop&#39;] += 1

            captions.update({key: caption})
        return captions

    def _video_time_to_str(self, video_time: float) -&gt; str:
        ss = int(video_time % 60)
        ttt = (video_time % 60 - ss) * 1000
        mm = video_time // 60
        return &#39;{:02.0f}:{:02.0f}.{:03.0f}&#39;.format(mm, ss, ttt)

    def _reading_to_str(self, reading: Reading) -&gt; str:
        if reading is None:
            return &#39;?&#39;
        return &#39;{x.value}{x.units}&#39;.format(x=reading)

    def _captions_dict_to_vtt(self, captions: Dict, framerate: float) -&gt; str:
        vtt = &#39;WEBVTT \n&#39;
        vtt += &#39;\n&#39;

        for key, caption in sorted(captions.items()):
            log: SensorLog = caption[&#39;log&#39;]
            if log is None:
                continue

            start_time = caption[&#39;start&#39;] / framerate
            stop_time = caption[&#39;stop&#39;] / framerate

            vtt += &#39;{} --&gt; {} - Sensor@{}\n&#39;.format(
                self._video_time_to_str(start_time),
                self._video_time_to_str(stop_time),
                &#39;{:%H:%M:%S}&#39;.format(
                    datetime.fromtimestamp(log.system_time, timezone.utc)
                ),
            )

            vtt += &#39;T: {} - RH: {} - L: {} - P: {}\n\n&#39;.format(
                self._reading_to_str(log.readings.get(&#39;SKEW_TEMPERATURE&#39;)),
                self._reading_to_str(log.readings.get(&#39;HUMIDITY&#39;)),
                self._reading_to_str(log.readings.get(&#39;BRIGHTNESS&#39;)),
                self._reading_to_str(log.readings.get(&#39;ATMOSPHERIC_PRESSURE&#39;)),
            )
        return vtt

    def generate_vtt_for(self, timestamps: List[float]) -&gt; StringIO:
        &#34;&#34;&#34;Generate WebVTT captions containing the sensor readings at given moments in time.

        Args:
            timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

        Returns:
            StringIO: The WebVTT captions ready to be sent to a server
        &#34;&#34;&#34;
        captions = self._generate_captions_dict(timestamps)
        return StringIO(self._captions_dict_to_vtt(captions, self._framerate))

    def generate_sensor_json(self, timestamps: List[float]) -&gt; StringIO:
        &#34;&#34;&#34;Generate JSON captions containing the sensor readings at given moments in time.

        The format is as follows:

        ```json
        [
            {
                &#34;start&#34;: 0,
                &#34;end&#34;: 1,
                &#34;log&#34;: {
                    &#34;EXAMPLE_SENSOR_1&#34;: {
                        &#34;value&#34;: 0.0,
                        &#34;units&#34;: &#34;x&#34;
                    },
                    &#34;EXAMPLE_SENSOR_2&#34;: {
                        &#34;value&#34;: 0.0,
                        &#34;units&#34;: &#34;x&#34;
                    }
                }
            },
            {
                &#34;start&#34;: 1,
                &#34;end&#34;: 5,
                &#34;logs&#34;: {}
            }
        ]
        ```

        The `&#34;start&#34;` and `&#34;end&#34;` correspond to the frame numbers in which the sensor logs are valid. The frame numbers are inclusive. It is not guaranteed that all frames are covered by logs. There may also be also be overlaps between entries if the exact timestamp where a new set of sensor readings becomes valid occurs during a frame.

        Args:
            timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

        Returns:
            StringIO: The JSON captions wrapped in a `StringIO`, ready for writing to file
        &#34;&#34;&#34;
        captions = self._generate_captions_dict(timestamps)
        logger.debug(captions)

        json_captions = []
        for c in captions.values():
            log = c[&#39;log&#39;]
            if log != None:
                json_captions.append(
                    {&#39;start&#39;: c[&#39;start&#39;], &#39;end&#39;: c[&#39;stop&#39;], &#39;log&#39;: log.serialise()}
                )
        return StringIO(dumps(json_captions))


class AbstractOutput:
    &#34;&#34;&#34;A base class to use for outputting captured images or videos. The `output_still()` and `output_video()` functions should be overridden with output method-specific implementations.&#34;&#34;&#34;

    def __init__(self, settings: OutputSettings, read_from: Tuple[Filter, SensorLogs]):
        self._frame_queue = read_from[0]
        self._sensor_logs = read_from[1]
        self.framerate = self._frame_queue.framerate

        if settings.output_format == OutputFormat.VIDEO:
            self._reader = Process(target=self._read_frames_to_video, daemon=True)
        else:
            self._reader = Process(target=self._read_frames, daemon=True)
        self._reader.start()

    def close(self):
        self._reader.terminate()
        self._reader.join()

    def _read_frames(self):
        while True:
            frame = self._frame_queue.get()
            if frame is None:
                continue

            log = self._sensor_logs.get(frame.timestamp)
            if log is None:
                logger.warning(&#39;No sensor readings&#39;)
                self.output_still(image=frame.image, time=frame.timestamp)
            else:
                self.output_still(
                    image=frame.image, time=frame.timestamp, sensor_log=log
                )

    def _read_frames_to_video(self):
        start_new = True
        start_time = 0
        caption_generator = VideoCaption(self._sensor_logs, self.framerate)
        while True:
            frame = self._frame_queue.get()

            # End of motion sequence
            if frame is None and not start_new:
                start_new = True
                writer.release()
                captions = caption_generator.generate_sensor_json(frame_timestamps)
                self.output_video(video=file, caption=captions, time=start_time)
                file.close()
                continue

            decoded_image = cv2.imdecode(asarray(frame.image), cv2.IMREAD_COLOR)

            # Start of motion sequence
            if start_new:
                start_new = False
                start_time = frame.timestamp
                frame_timestamps = []
                file = NamedTemporaryFile(suffix=&#39;.mp4&#39;)

                writer = cv2.VideoWriter(
                    file.name,
                    cv2.VideoWriter_fourcc(*&#39;avc1&#39;),
                    self.framerate,
                    (decoded_image.shape[1], decoded_image.shape[0]),
                )

            writer.write(decoded_image)
            frame_timestamps.append(frame.timestamp)

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):
        &#34;&#34;&#34;Output a still image with its sensor data. The sensor data can be provided via the keyword arguments.

        Args:
            image (bytes): The JPEG image frame
            time (float): UNIX timestamp when the image was captured
            sensor_log (SensorLog): Log of sensor values at time frame was captured

        Raises:
            NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        &#34;&#34;&#34;Output a video with its meta-data. The sensor data is provided via the video captions (`caption`).

        Args:
            video (IO[bytes]): MP4 video (codec: H264 - MPEG-4 AVC (part 10))
            caption (StringIO): Caption of sensor readings as produced by `VideoCaption.generate_sensor_json()`
            time (float): UNIX timestamp when the image was captured

        Raises:
            NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
        &#34;&#34;&#34;
        raise NotImplementedError()


class Sender(AbstractOutput):
    &#34;&#34;&#34;The Sender is a simple interface for sending the desired data to a server&#34;&#34;&#34;

    def __init__(self, settings: SenderSettings, read_from: Tuple[Filter, SensorLogs]):
        self._server = settings.server
        self._device_id = settings.device_id
        self._path_POST = settings.POST
        super().__init__(settings, read_from)

        logger.debug(&#39;Sender started (format: {})&#39;.format(settings.output_format))

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):

        files_dict = {&#39;file&#39;: (&#39;image&#39;, image, &#39;image/jpeg&#39;)}
        logger.debug(&#39;Sending capture, meta = {}&#39;.format(sensor_log))
        try:
            r = post(self._server + self._path_POST, data=sensor_log, files=files_dict)
            r.raise_for_status()
            logger.info(&#39;Image sent&#39;)
        except HTTPError as e:
            logger.error(e)
        except ConnectionError as e:
            logger.error(&#39;Connection to server failed; could not send data&#39;)

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        meta = {&#39;trap_id&#39;: self._device_id, &#39;time&#39;: time}
        files_dict = {
            &#39;video&#39;: (&#39;video&#39;, video, &#39;video/mp4&#39;),
            &#39;caption&#39;: (&#39;caption&#39;, caption, &#39;text/vtt&#39;),
        }
        try:
            r = post(self._server + self._path_POST, data=meta, files=files_dict)
            r.raise_for_status()
            logger.info(&#39;Video sent&#39;)
        except HTTPError as e:
            logger.error(e)
        except ConnectionError as e:
            logger.error(&#39;Connection to server failed; could not send data&#39;)


class Writer(AbstractOutput):
    def __init__(self, settings: WriterSettings, read_from: Tuple[Filter, SensorLogs]):

        path = Path(settings.path).expanduser()
        path.mkdir(parents=True, exist_ok=True)
        self._path = path.resolve()

        super().__init__(settings, read_from)
        logger.debug(&#39;Writer started (format: {})&#39;.format(settings.output_format))

    def _unique_name(self, capture_time: float) -&gt; str:

        # Get all filenames and remove extensions
        names = map(lambda x: x[0], map(lambda x: x.split(&#39;.&#39;), listdir(self._path)))

        # Base the new file&#39;s name on the capture time
        name = &#39;{:%Y-%m-%d_%H-%M-%S-%f}&#39;.format(
            datetime.fromtimestamp(capture_time, timezone.utc)
        )
        counter = 0

        # If the name is already taken try adding a number
        while &#39;{}_{}&#39;.format(name, counter) in list(names):
            counter += 1

        name = &#39;{}_{}&#39;.format(name, counter)

        return join(self._path, name)

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):

        name = self._unique_name(time)

        with open(name + &#39;.jpg&#39;, &#39;wb&#39;) as f:
            f.write(image)

        with open(name + &#39;.json&#39;, &#39;w&#39;) as f:
            dump(sensor_log.serialise(), f)
        logger.info(&#39;Image and meta-data saved&#39;)

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        name = self._unique_name(time)

        with open(name + &#39;.mp4&#39;, &#39;wb&#39;) as f:
            f.write(video.read())

        with open(name + &#39;.json&#39;, &#39;w&#39;) as f:
            f.write(caption.getvalue())

        logger.info(&#39;Video and caption saved&#39;)


def Output(
    settings: OutputSettings, read_from: Tuple[Filter, SensorLogs]
) -&gt; Union[Sender, Writer]:
    &#34;&#34;&#34;Generator function to provide an implementation of the `AbstractOutput` based on the `DynAIkonTrap.settings.OutputMode` of the `settings` argument.&#34;&#34;&#34;
    if settings.output.output_mode == OutputMode.SEND:
        Sender(settings=settings.output, read_from=read_from)
    else:
        Writer(settings=settings.output, read_from=read_from)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="DynAIkonTrap.comms.Output"><code class="name flex">
<span>def <span class="ident">Output</span></span>(<span>settings:Â <a title="DynAIkonTrap.settings.OutputSettings" href="settings.html#DynAIkonTrap.settings.OutputSettings">OutputSettings</a>, read_from:Â Tuple[<a title="DynAIkonTrap.filtering.filtering.Filter" href="filtering/filtering.html#DynAIkonTrap.filtering.filtering.Filter">Filter</a>,Â <a title="DynAIkonTrap.sensor.SensorLogs" href="sensor.html#DynAIkonTrap.sensor.SensorLogs">SensorLogs</a>]) â€‘>Â Union[<a title="DynAIkonTrap.comms.Sender" href="#DynAIkonTrap.comms.Sender">Sender</a>,Â <a title="DynAIkonTrap.comms.Writer" href="#DynAIkonTrap.comms.Writer">Writer</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Generator function to provide an implementation of the <code><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></code> based on the <code><a title="DynAIkonTrap.settings.OutputMode" href="settings.html#DynAIkonTrap.settings.OutputMode">OutputMode</a></code> of the <code>settings</code> argument.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Output(
    settings: OutputSettings, read_from: Tuple[Filter, SensorLogs]
) -&gt; Union[Sender, Writer]:
    &#34;&#34;&#34;Generator function to provide an implementation of the `AbstractOutput` based on the `DynAIkonTrap.settings.OutputMode` of the `settings` argument.&#34;&#34;&#34;
    if settings.output.output_mode == OutputMode.SEND:
        Sender(settings=settings.output, read_from=read_from)
    else:
        Writer(settings=settings.output, read_from=read_from)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="DynAIkonTrap.comms.AbstractOutput"><code class="flex name class">
<span>class <span class="ident">AbstractOutput</span></span>
<span>(</span><span>settings:Â <a title="DynAIkonTrap.settings.OutputSettings" href="settings.html#DynAIkonTrap.settings.OutputSettings">OutputSettings</a>, read_from:Â Tuple[<a title="DynAIkonTrap.filtering.filtering.Filter" href="filtering/filtering.html#DynAIkonTrap.filtering.filtering.Filter">Filter</a>,Â <a title="DynAIkonTrap.sensor.SensorLogs" href="sensor.html#DynAIkonTrap.sensor.SensorLogs">SensorLogs</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>A base class to use for outputting captured images or videos. The <code>output_still()</code> and <code>output_video()</code> functions should be overridden with output method-specific implementations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractOutput:
    &#34;&#34;&#34;A base class to use for outputting captured images or videos. The `output_still()` and `output_video()` functions should be overridden with output method-specific implementations.&#34;&#34;&#34;

    def __init__(self, settings: OutputSettings, read_from: Tuple[Filter, SensorLogs]):
        self._frame_queue = read_from[0]
        self._sensor_logs = read_from[1]
        self.framerate = self._frame_queue.framerate

        if settings.output_format == OutputFormat.VIDEO:
            self._reader = Process(target=self._read_frames_to_video, daemon=True)
        else:
            self._reader = Process(target=self._read_frames, daemon=True)
        self._reader.start()

    def close(self):
        self._reader.terminate()
        self._reader.join()

    def _read_frames(self):
        while True:
            frame = self._frame_queue.get()
            if frame is None:
                continue

            log = self._sensor_logs.get(frame.timestamp)
            if log is None:
                logger.warning(&#39;No sensor readings&#39;)
                self.output_still(image=frame.image, time=frame.timestamp)
            else:
                self.output_still(
                    image=frame.image, time=frame.timestamp, sensor_log=log
                )

    def _read_frames_to_video(self):
        start_new = True
        start_time = 0
        caption_generator = VideoCaption(self._sensor_logs, self.framerate)
        while True:
            frame = self._frame_queue.get()

            # End of motion sequence
            if frame is None and not start_new:
                start_new = True
                writer.release()
                captions = caption_generator.generate_sensor_json(frame_timestamps)
                self.output_video(video=file, caption=captions, time=start_time)
                file.close()
                continue

            decoded_image = cv2.imdecode(asarray(frame.image), cv2.IMREAD_COLOR)

            # Start of motion sequence
            if start_new:
                start_new = False
                start_time = frame.timestamp
                frame_timestamps = []
                file = NamedTemporaryFile(suffix=&#39;.mp4&#39;)

                writer = cv2.VideoWriter(
                    file.name,
                    cv2.VideoWriter_fourcc(*&#39;avc1&#39;),
                    self.framerate,
                    (decoded_image.shape[1], decoded_image.shape[0]),
                )

            writer.write(decoded_image)
            frame_timestamps.append(frame.timestamp)

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):
        &#34;&#34;&#34;Output a still image with its sensor data. The sensor data can be provided via the keyword arguments.

        Args:
            image (bytes): The JPEG image frame
            time (float): UNIX timestamp when the image was captured
            sensor_log (SensorLog): Log of sensor values at time frame was captured

        Raises:
            NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        &#34;&#34;&#34;Output a video with its meta-data. The sensor data is provided via the video captions (`caption`).

        Args:
            video (IO[bytes]): MP4 video (codec: H264 - MPEG-4 AVC (part 10))
            caption (StringIO): Caption of sensor readings as produced by `VideoCaption.generate_sensor_json()`
            time (float): UNIX timestamp when the image was captured

        Raises:
            NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
        &#34;&#34;&#34;
        raise NotImplementedError()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="DynAIkonTrap.comms.Sender" href="#DynAIkonTrap.comms.Sender">Sender</a></li>
<li><a title="DynAIkonTrap.comms.Writer" href="#DynAIkonTrap.comms.Writer">Writer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="DynAIkonTrap.comms.AbstractOutput.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    self._reader.terminate()
    self._reader.join()</code></pre>
</details>
</dd>
<dt id="DynAIkonTrap.comms.AbstractOutput.output_still"><code class="name flex">
<span>def <span class="ident">output_still</span></span>(<span>self, image:Â bytes, time:Â float, sensor_log:Â <a title="DynAIkonTrap.sensor.SensorLog" href="sensor.html#DynAIkonTrap.sensor.SensorLog">SensorLog</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Output a still image with its sensor data. The sensor data can be provided via the keyword arguments.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>bytes</code></dt>
<dd>The JPEG image frame</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>float</code></dt>
<dd>UNIX timestamp when the image was captured</dd>
<dt><strong><code>sensor_log</code></strong> :&ensp;<code>SensorLog</code></dt>
<dd>Log of sensor values at time frame was captured</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>A subclass should implement this function for the specific use-case e.g. writing to disk.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output_still(self, image: bytes, time: float, sensor_log: SensorLog):
    &#34;&#34;&#34;Output a still image with its sensor data. The sensor data can be provided via the keyword arguments.

    Args:
        image (bytes): The JPEG image frame
        time (float): UNIX timestamp when the image was captured
        sensor_log (SensorLog): Log of sensor values at time frame was captured

    Raises:
        NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="DynAIkonTrap.comms.AbstractOutput.output_video"><code class="name flex">
<span>def <span class="ident">output_video</span></span>(<span>self, video:Â IO[bytes], caption:Â _io.StringIO, time:Â float, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Output a video with its meta-data. The sensor data is provided via the video captions (<code>caption</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>video</code></strong> :&ensp;<code>IO[bytes]</code></dt>
<dd>MP4 video (codec: H264 - MPEG-4 AVC (part 10))</dd>
<dt><strong><code>caption</code></strong> :&ensp;<code>StringIO</code></dt>
<dd>Caption of sensor readings as produced by <code><a title="DynAIkonTrap.comms.VideoCaption.generate_sensor_json" href="#DynAIkonTrap.comms.VideoCaption.generate_sensor_json">VideoCaption.generate_sensor_json()</a></code></dd>
<dt><strong><code>time</code></strong> :&ensp;<code>float</code></dt>
<dd>UNIX timestamp when the image was captured</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>A subclass should implement this function for the specific use-case e.g. writing to disk.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
    &#34;&#34;&#34;Output a video with its meta-data. The sensor data is provided via the video captions (`caption`).

    Args:
        video (IO[bytes]): MP4 video (codec: H264 - MPEG-4 AVC (part 10))
        caption (StringIO): Caption of sensor readings as produced by `VideoCaption.generate_sensor_json()`
        time (float): UNIX timestamp when the image was captured

    Raises:
        NotImplementedError: A subclass should implement this function for the specific use-case e.g. writing to disk.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="DynAIkonTrap.comms.Sender"><code class="flex name class">
<span>class <span class="ident">Sender</span></span>
<span>(</span><span>settings:Â <a title="DynAIkonTrap.settings.SenderSettings" href="settings.html#DynAIkonTrap.settings.SenderSettings">SenderSettings</a>, read_from:Â Tuple[<a title="DynAIkonTrap.filtering.filtering.Filter" href="filtering/filtering.html#DynAIkonTrap.filtering.filtering.Filter">Filter</a>,Â <a title="DynAIkonTrap.sensor.SensorLogs" href="sensor.html#DynAIkonTrap.sensor.SensorLogs">SensorLogs</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>The Sender is a simple interface for sending the desired data to a server</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sender(AbstractOutput):
    &#34;&#34;&#34;The Sender is a simple interface for sending the desired data to a server&#34;&#34;&#34;

    def __init__(self, settings: SenderSettings, read_from: Tuple[Filter, SensorLogs]):
        self._server = settings.server
        self._device_id = settings.device_id
        self._path_POST = settings.POST
        super().__init__(settings, read_from)

        logger.debug(&#39;Sender started (format: {})&#39;.format(settings.output_format))

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):

        files_dict = {&#39;file&#39;: (&#39;image&#39;, image, &#39;image/jpeg&#39;)}
        logger.debug(&#39;Sending capture, meta = {}&#39;.format(sensor_log))
        try:
            r = post(self._server + self._path_POST, data=sensor_log, files=files_dict)
            r.raise_for_status()
            logger.info(&#39;Image sent&#39;)
        except HTTPError as e:
            logger.error(e)
        except ConnectionError as e:
            logger.error(&#39;Connection to server failed; could not send data&#39;)

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        meta = {&#39;trap_id&#39;: self._device_id, &#39;time&#39;: time}
        files_dict = {
            &#39;video&#39;: (&#39;video&#39;, video, &#39;video/mp4&#39;),
            &#39;caption&#39;: (&#39;caption&#39;, caption, &#39;text/vtt&#39;),
        }
        try:
            r = post(self._server + self._path_POST, data=meta, files=files_dict)
            r.raise_for_status()
            logger.info(&#39;Video sent&#39;)
        except HTTPError as e:
            logger.error(e)
        except ConnectionError as e:
            logger.error(&#39;Connection to server failed; could not send data&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></b></code>:
<ul class="hlist">
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_still" href="#DynAIkonTrap.comms.AbstractOutput.output_still">output_still</a></code></li>
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_video" href="#DynAIkonTrap.comms.AbstractOutput.output_video">output_video</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="DynAIkonTrap.comms.VideoCaption"><code class="flex name class">
<span>class <span class="ident">VideoCaption</span></span>
<span>(</span><span>sensor_logs:Â <a title="DynAIkonTrap.sensor.SensorLogs" href="sensor.html#DynAIkonTrap.sensor.SensorLogs">SensorLogs</a>, framerate:Â float)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to aid in generating captions for video output. The captions are based on the logged sensor readings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sensor_logs</code></strong> :&ensp;<code>SensorLogs</code></dt>
<dd>The object containing the log of sensor readings</dd>
<dt><strong><code>framerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Camera framerate</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoCaption:
    &#34;&#34;&#34;Class to aid in generating captions for video output. The captions are based on the logged sensor readings.&#34;&#34;&#34;

    def __init__(self, sensor_logs: SensorLogs, framerate: float):
        &#34;&#34;&#34;
        Args:
            sensor_logs (SensorLogs): The object containing the log of sensor readings
            framerate (float): Camera framerate
        &#34;&#34;&#34;
        self._sensor_logs = sensor_logs
        self._framerate = framerate

    def _generate_captions_dict(self, timestamps: List[float]) -&gt; Dict:
        &#34;&#34;&#34;NOTE: If sensor readings do not line up with frame capturing, there may be slight off-by-one style errors.&#34;&#34;&#34;
        captions = {}
        for frame_number, timestamp in enumerate(timestamps):
            # Retrieve the corresponding log
            log = self._sensor_logs.get(timestamp)

            # Get existing or create new caption with this log
            key = int(
                frame_number // (self._framerate * self._sensor_logs.read_interval)
            )
            caption = captions.get(
                key,
                {
                    &#39;start&#39;: frame_number,
                    &#39;stop&#39;: frame_number,
                    &#39;log&#39;: log,
                },
            )

            # Extend caption duration for a subsequent frame using the same log
            caption[&#39;stop&#39;] += 1

            captions.update({key: caption})
        return captions

    def _video_time_to_str(self, video_time: float) -&gt; str:
        ss = int(video_time % 60)
        ttt = (video_time % 60 - ss) * 1000
        mm = video_time // 60
        return &#39;{:02.0f}:{:02.0f}.{:03.0f}&#39;.format(mm, ss, ttt)

    def _reading_to_str(self, reading: Reading) -&gt; str:
        if reading is None:
            return &#39;?&#39;
        return &#39;{x.value}{x.units}&#39;.format(x=reading)

    def _captions_dict_to_vtt(self, captions: Dict, framerate: float) -&gt; str:
        vtt = &#39;WEBVTT \n&#39;
        vtt += &#39;\n&#39;

        for key, caption in sorted(captions.items()):
            log: SensorLog = caption[&#39;log&#39;]
            if log is None:
                continue

            start_time = caption[&#39;start&#39;] / framerate
            stop_time = caption[&#39;stop&#39;] / framerate

            vtt += &#39;{} --&gt; {} - Sensor@{}\n&#39;.format(
                self._video_time_to_str(start_time),
                self._video_time_to_str(stop_time),
                &#39;{:%H:%M:%S}&#39;.format(
                    datetime.fromtimestamp(log.system_time, timezone.utc)
                ),
            )

            vtt += &#39;T: {} - RH: {} - L: {} - P: {}\n\n&#39;.format(
                self._reading_to_str(log.readings.get(&#39;SKEW_TEMPERATURE&#39;)),
                self._reading_to_str(log.readings.get(&#39;HUMIDITY&#39;)),
                self._reading_to_str(log.readings.get(&#39;BRIGHTNESS&#39;)),
                self._reading_to_str(log.readings.get(&#39;ATMOSPHERIC_PRESSURE&#39;)),
            )
        return vtt

    def generate_vtt_for(self, timestamps: List[float]) -&gt; StringIO:
        &#34;&#34;&#34;Generate WebVTT captions containing the sensor readings at given moments in time.

        Args:
            timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

        Returns:
            StringIO: The WebVTT captions ready to be sent to a server
        &#34;&#34;&#34;
        captions = self._generate_captions_dict(timestamps)
        return StringIO(self._captions_dict_to_vtt(captions, self._framerate))

    def generate_sensor_json(self, timestamps: List[float]) -&gt; StringIO:
        &#34;&#34;&#34;Generate JSON captions containing the sensor readings at given moments in time.

        The format is as follows:

        ```json
        [
            {
                &#34;start&#34;: 0,
                &#34;end&#34;: 1,
                &#34;log&#34;: {
                    &#34;EXAMPLE_SENSOR_1&#34;: {
                        &#34;value&#34;: 0.0,
                        &#34;units&#34;: &#34;x&#34;
                    },
                    &#34;EXAMPLE_SENSOR_2&#34;: {
                        &#34;value&#34;: 0.0,
                        &#34;units&#34;: &#34;x&#34;
                    }
                }
            },
            {
                &#34;start&#34;: 1,
                &#34;end&#34;: 5,
                &#34;logs&#34;: {}
            }
        ]
        ```

        The `&#34;start&#34;` and `&#34;end&#34;` correspond to the frame numbers in which the sensor logs are valid. The frame numbers are inclusive. It is not guaranteed that all frames are covered by logs. There may also be also be overlaps between entries if the exact timestamp where a new set of sensor readings becomes valid occurs during a frame.

        Args:
            timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

        Returns:
            StringIO: The JSON captions wrapped in a `StringIO`, ready for writing to file
        &#34;&#34;&#34;
        captions = self._generate_captions_dict(timestamps)
        logger.debug(captions)

        json_captions = []
        for c in captions.values():
            log = c[&#39;log&#39;]
            if log != None:
                json_captions.append(
                    {&#39;start&#39;: c[&#39;start&#39;], &#39;end&#39;: c[&#39;stop&#39;], &#39;log&#39;: log.serialise()}
                )
        return StringIO(dumps(json_captions))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="DynAIkonTrap.comms.VideoCaption.generate_sensor_json"><code class="name flex">
<span>def <span class="ident">generate_sensor_json</span></span>(<span>self, timestamps:Â List[float]) â€‘>Â _io.StringIO</span>
</code></dt>
<dd>
<div class="desc"><p>Generate JSON captions containing the sensor readings at given moments in time.</p>
<p>The format is as follows:</p>
<pre><code class="language-json">[
    {
        &quot;start&quot;: 0,
        &quot;end&quot;: 1,
        &quot;log&quot;: {
            &quot;EXAMPLE_SENSOR_1&quot;: {
                &quot;value&quot;: 0.0,
                &quot;units&quot;: &quot;x&quot;
            },
            &quot;EXAMPLE_SENSOR_2&quot;: {
                &quot;value&quot;: 0.0,
                &quot;units&quot;: &quot;x&quot;
            }
        }
    },
    {
        &quot;start&quot;: 1,
        &quot;end&quot;: 5,
        &quot;logs&quot;: {}
    }
]
</code></pre>
<p>The <code>"start"</code> and <code>"end"</code> correspond to the frame numbers in which the sensor logs are valid. The frame numbers are inclusive. It is not guaranteed that all frames are covered by logs. There may also be also be overlaps between entries if the exact timestamp where a new set of sensor readings becomes valid occurs during a frame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamps</code></strong> :&ensp;<code>List[float]</code></dt>
<dd>Timestamps for every frame in the motion/animal sequence</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>StringIO</code></dt>
<dd>The JSON captions wrapped in a <code>StringIO</code>, ready for writing to file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_sensor_json(self, timestamps: List[float]) -&gt; StringIO:
    &#34;&#34;&#34;Generate JSON captions containing the sensor readings at given moments in time.

    The format is as follows:

    ```json
    [
        {
            &#34;start&#34;: 0,
            &#34;end&#34;: 1,
            &#34;log&#34;: {
                &#34;EXAMPLE_SENSOR_1&#34;: {
                    &#34;value&#34;: 0.0,
                    &#34;units&#34;: &#34;x&#34;
                },
                &#34;EXAMPLE_SENSOR_2&#34;: {
                    &#34;value&#34;: 0.0,
                    &#34;units&#34;: &#34;x&#34;
                }
            }
        },
        {
            &#34;start&#34;: 1,
            &#34;end&#34;: 5,
            &#34;logs&#34;: {}
        }
    ]
    ```

    The `&#34;start&#34;` and `&#34;end&#34;` correspond to the frame numbers in which the sensor logs are valid. The frame numbers are inclusive. It is not guaranteed that all frames are covered by logs. There may also be also be overlaps between entries if the exact timestamp where a new set of sensor readings becomes valid occurs during a frame.

    Args:
        timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

    Returns:
        StringIO: The JSON captions wrapped in a `StringIO`, ready for writing to file
    &#34;&#34;&#34;
    captions = self._generate_captions_dict(timestamps)
    logger.debug(captions)

    json_captions = []
    for c in captions.values():
        log = c[&#39;log&#39;]
        if log != None:
            json_captions.append(
                {&#39;start&#39;: c[&#39;start&#39;], &#39;end&#39;: c[&#39;stop&#39;], &#39;log&#39;: log.serialise()}
            )
    return StringIO(dumps(json_captions))</code></pre>
</details>
</dd>
<dt id="DynAIkonTrap.comms.VideoCaption.generate_vtt_for"><code class="name flex">
<span>def <span class="ident">generate_vtt_for</span></span>(<span>self, timestamps:Â List[float]) â€‘>Â _io.StringIO</span>
</code></dt>
<dd>
<div class="desc"><p>Generate WebVTT captions containing the sensor readings at given moments in time.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timestamps</code></strong> :&ensp;<code>List[float]</code></dt>
<dd>Timestamps for every frame in the motion/animal sequence</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>StringIO</code></dt>
<dd>The WebVTT captions ready to be sent to a server</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_vtt_for(self, timestamps: List[float]) -&gt; StringIO:
    &#34;&#34;&#34;Generate WebVTT captions containing the sensor readings at given moments in time.

    Args:
        timestamps (List[float]): Timestamps for every frame in the motion/animal sequence

    Returns:
        StringIO: The WebVTT captions ready to be sent to a server
    &#34;&#34;&#34;
    captions = self._generate_captions_dict(timestamps)
    return StringIO(self._captions_dict_to_vtt(captions, self._framerate))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="DynAIkonTrap.comms.Writer"><code class="flex name class">
<span>class <span class="ident">Writer</span></span>
<span>(</span><span>settings:Â <a title="DynAIkonTrap.settings.WriterSettings" href="settings.html#DynAIkonTrap.settings.WriterSettings">WriterSettings</a>, read_from:Â Tuple[<a title="DynAIkonTrap.filtering.filtering.Filter" href="filtering/filtering.html#DynAIkonTrap.filtering.filtering.Filter">Filter</a>,Â <a title="DynAIkonTrap.sensor.SensorLogs" href="sensor.html#DynAIkonTrap.sensor.SensorLogs">SensorLogs</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>A base class to use for outputting captured images or videos. The <code>output_still()</code> and <code>output_video()</code> functions should be overridden with output method-specific implementations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Writer(AbstractOutput):
    def __init__(self, settings: WriterSettings, read_from: Tuple[Filter, SensorLogs]):

        path = Path(settings.path).expanduser()
        path.mkdir(parents=True, exist_ok=True)
        self._path = path.resolve()

        super().__init__(settings, read_from)
        logger.debug(&#39;Writer started (format: {})&#39;.format(settings.output_format))

    def _unique_name(self, capture_time: float) -&gt; str:

        # Get all filenames and remove extensions
        names = map(lambda x: x[0], map(lambda x: x.split(&#39;.&#39;), listdir(self._path)))

        # Base the new file&#39;s name on the capture time
        name = &#39;{:%Y-%m-%d_%H-%M-%S-%f}&#39;.format(
            datetime.fromtimestamp(capture_time, timezone.utc)
        )
        counter = 0

        # If the name is already taken try adding a number
        while &#39;{}_{}&#39;.format(name, counter) in list(names):
            counter += 1

        name = &#39;{}_{}&#39;.format(name, counter)

        return join(self._path, name)

    def output_still(self, image: bytes, time: float, sensor_log: SensorLog):

        name = self._unique_name(time)

        with open(name + &#39;.jpg&#39;, &#39;wb&#39;) as f:
            f.write(image)

        with open(name + &#39;.json&#39;, &#39;w&#39;) as f:
            dump(sensor_log.serialise(), f)
        logger.info(&#39;Image and meta-data saved&#39;)

    def output_video(self, video: IO[bytes], caption: StringIO, time: float, **kwargs):
        name = self._unique_name(time)

        with open(name + &#39;.mp4&#39;, &#39;wb&#39;) as f:
            f.write(video.read())

        with open(name + &#39;.json&#39;, &#39;w&#39;) as f:
            f.write(caption.getvalue())

        logger.info(&#39;Video and caption saved&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></b></code>:
<ul class="hlist">
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_still" href="#DynAIkonTrap.comms.AbstractOutput.output_still">output_still</a></code></li>
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_video" href="#DynAIkonTrap.comms.AbstractOutput.output_video">output_video</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="DynAIkonTrap" href="index.html">DynAIkonTrap</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="DynAIkonTrap.comms.Output" href="#DynAIkonTrap.comms.Output">Output</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="DynAIkonTrap.comms.AbstractOutput" href="#DynAIkonTrap.comms.AbstractOutput">AbstractOutput</a></code></h4>
<ul class="">
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.close" href="#DynAIkonTrap.comms.AbstractOutput.close">close</a></code></li>
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_still" href="#DynAIkonTrap.comms.AbstractOutput.output_still">output_still</a></code></li>
<li><code><a title="DynAIkonTrap.comms.AbstractOutput.output_video" href="#DynAIkonTrap.comms.AbstractOutput.output_video">output_video</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DynAIkonTrap.comms.Sender" href="#DynAIkonTrap.comms.Sender">Sender</a></code></h4>
</li>
<li>
<h4><code><a title="DynAIkonTrap.comms.VideoCaption" href="#DynAIkonTrap.comms.VideoCaption">VideoCaption</a></code></h4>
<ul class="">
<li><code><a title="DynAIkonTrap.comms.VideoCaption.generate_sensor_json" href="#DynAIkonTrap.comms.VideoCaption.generate_sensor_json">generate_sensor_json</a></code></li>
<li><code><a title="DynAIkonTrap.comms.VideoCaption.generate_vtt_for" href="#DynAIkonTrap.comms.VideoCaption.generate_vtt_for">generate_vtt_for</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="DynAIkonTrap.comms.Writer" href="#DynAIkonTrap.comms.Writer">Writer</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>